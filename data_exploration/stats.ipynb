{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from fastai.text import SpacyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOS_CLASS = 'NO_NODES_DETECTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path('../train/')\n",
    "test_dir = Path('../test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg',  disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/ubuntu/gaurav/.fastai/models/wt103-fwd/itos_wt103.pkl\", \"rb\") as input_file:\n",
    "    wiki_vocab = set(pickle.load(input_file))\n",
    "print(len(wiki_vocab))\n",
    "tokenizer = SpacyTokenizer('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a: List[str], b: List[str], n: int = 3) -> float:\n",
    "    answer = 1.0\n",
    "    for i in range(1, n + 1):\n",
    "        a_i = set(nltk.ngrams(a, i))\n",
    "        b_i = set(nltk.ngrams(b, i))\n",
    "        intersection = len(a_i & b_i)\n",
    "        union = len(a_i | b_i)\n",
    "        if union:\n",
    "            answer -= (intersection / union)\n",
    "    return answer\n",
    "#     return max(0, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity(train_df: pd.DataFrame) -> float:\n",
    "    div, labelsc = 0, 0\n",
    "    for label, group_df in train_df.groupby('label'):\n",
    "        acc = 0\n",
    "        labelsc += 1\n",
    "        for text_a in group_df['sentence_tokens']:\n",
    "            for text_b in group_df['sentence_tokens']:\n",
    "                d = dist(text_a, text_b)\n",
    "                acc += d\n",
    "        div += (acc / (len(group_df) * len(group_df)))\n",
    "    return div / labelsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(train_df: pd.DataFrame, test_df: pd.DataFrame) -> float:\n",
    "    cov, labelsc = 0, 0\n",
    "    for label, group_df in test_df.groupby('label'):\n",
    "        if label == OOS_CLASS:\n",
    "            continue\n",
    "        labelsc += 1\n",
    "        train_group = train_df[train_df['label'] == label]\n",
    "        acc = 0\n",
    "        for text_b in group_df['sentence_tokens']:\n",
    "            acc += max(1.0 - dist(text_a, text_b) for text_a in train_group['sentence_tokens'])\n",
    "        cov += (acc / len(group_df))\n",
    "    return cov / labelsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path: Path) -> pd.DataFrame:\n",
    "    print(f'Reading file {path}')\n",
    "    df = pd.read_csv(str(path))\n",
    "    df['sentence_tokens'] = df['sentence'].apply(lambda sent: [tok.text for tok in nlp(sent.lower().strip())])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stats(df):\n",
    "    all_toks = set()\n",
    "    oov_toks = set()\n",
    "    tok_lens = []\n",
    "    for sentence in df['sentence']:\n",
    "        doc = tokenizer.tokenizer(sentence.lower().strip())\n",
    "        tok_lens.append(len(doc))\n",
    "        for tok in doc:\n",
    "            all_toks.add(tok)\n",
    "            if tok not in wiki_vocab:\n",
    "                oov_toks.add(tok)\n",
    "    return {\n",
    "        'len': len(df),\n",
    "        'in-scope': len(df[df['label'] != OOS_CLASS]),\n",
    "        'oos': len(df[df['label'] == OOS_CLASS]),\n",
    "        'labels': len(df[df['label'] != OOS_CLASS]['label'].unique()),\n",
    "        'tok_min': min(tok_lens),\n",
    "        'tok_max': max(tok_lens),\n",
    "        'tok_mean': np.mean(tok_lens),\n",
    "        'tok_std': np.std(tok_lens),\n",
    "        'oov_percentage': len(oov_toks) / len(all_toks),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ../train/sofmattress_train.csv\n",
      "Reading file ../test/sofmattress_test.csv\n",
      "train stats: {'len': 328, 'in-scope': 328, 'oos': 0, 'labels': 21, 'tok_min': 1, 'tok_max': 28, 'tok_mean': 4.414634146341464, 'tok_std': 2.542090648688811, 'oov_percentage': 0.05084745762711865}\n",
      "test stats: {'len': 397, 'in-scope': 231, 'oos': 166, 'labels': 20, 'tok_min': 1, 'tok_max': 53, 'tok_mean': 6.607052896725441, 'tok_std': 5.770746222932882, 'oov_percentage': 0.20937042459736457}\n",
      "Diversity: 0.6168521547770577\n",
      "Coverage: 0.4411860589631133\n",
      "Reading file ../train/sofmattress_subset_train.csv\n",
      "Reading file ../test/sofmattress_test.csv\n",
      "train stats: {'len': 180, 'in-scope': 180, 'oos': 0, 'labels': 21, 'tok_min': 1, 'tok_max': 28, 'tok_mean': 5.338888888888889, 'tok_std': 2.828749014609518, 'oov_percentage': 0.049429657794676805}\n",
      "test stats: {'len': 397, 'in-scope': 231, 'oos': 166, 'labels': 20, 'tok_min': 1, 'tok_max': 53, 'tok_mean': 6.607052896725441, 'tok_std': 5.770746222932882, 'oov_percentage': 0.20937042459736457}\n",
      "Diversity: 0.44642491013275915\n",
      "Coverage: 0.3599558172984878\n",
      "Reading file ../train/powerplay11_train.csv\n",
      "Reading file ../test/powerplay11_test.csv\n",
      "train stats: {'len': 471, 'in-scope': 471, 'oos': 0, 'labels': 59, 'tok_min': 1, 'tok_max': 31, 'tok_mean': 5.021231422505308, 'tok_std': 3.5458141003642187, 'oov_percentage': 0.05102040816326531}\n",
      "test stats: {'len': 983, 'in-scope': 275, 'oos': 708, 'labels': 58, 'tok_min': 1, 'tok_max': 73, 'tok_mean': 7.2868769074262465, 'tok_std': 6.8174198446968575, 'oov_percentage': 0.3639822447685479}\n",
      "Diversity: -0.014979018179958047\n",
      "Coverage: 0.507159939637793\n",
      "Reading file ../train/powerplay11_subset_train.csv\n",
      "Reading file ../test/powerplay11_test.csv\n",
      "train stats: {'len': 261, 'in-scope': 261, 'oos': 0, 'labels': 59, 'tok_min': 1, 'tok_max': 31, 'tok_mean': 6.0344827586206895, 'tok_std': 4.166409028895613, 'oov_percentage': 0.05070422535211268}\n",
      "test stats: {'len': 983, 'in-scope': 275, 'oos': 708, 'labels': 58, 'tok_min': 1, 'tok_max': 73, 'tok_mean': 7.2868769074262465, 'tok_std': 6.8174198446968575, 'oov_percentage': 0.3639822447685479}\n",
      "Diversity: -0.3480264198264641\n",
      "Coverage: 0.4201842473694509\n",
      "Reading file ../train/curekart_train.csv\n",
      "Reading file ../test/curekart_test.csv\n",
      "train stats: {'len': 600, 'in-scope': 600, 'oos': 0, 'labels': 28, 'tok_min': 1, 'tok_max': 27, 'tok_mean': 6.29, 'tok_std': 4.032273965560045, 'oov_percentage': 0.10204081632653061}\n",
      "test stats: {'len': 991, 'in-scope': 452, 'oos': 539, 'labels': 21, 'tok_min': 1, 'tok_max': 44, 'tok_mean': 6.424823410696266, 'tok_std': 5.02407416696985, 'oov_percentage': 0.30474934036939316}\n",
      "Diversity: 0.5431161766997563\n",
      "Coverage: 0.7164532041212871\n",
      "Reading file ../train/curekart_subset_train.csv\n",
      "Reading file ../test/curekart_test.csv\n",
      "train stats: {'len': 413, 'in-scope': 413, 'oos': 0, 'labels': 28, 'tok_min': 1, 'tok_max': 27, 'tok_mean': 7.196125907990314, 'tok_std': 4.236676538359367, 'oov_percentage': 0.09433962264150944}\n",
      "test stats: {'len': 991, 'in-scope': 452, 'oos': 539, 'labels': 21, 'tok_min': 1, 'tok_max': 44, 'tok_mean': 6.424823410696266, 'tok_std': 5.02407416696985, 'oov_percentage': 0.30474934036939316}\n",
      "Diversity: 0.4406516266397046\n",
      "Coverage: 0.5727839744833852\n"
     ]
    }
   ],
   "source": [
    "datasets = ['sofmattress', 'powerplay11', 'curekart']\n",
    "for dataset in datasets:\n",
    "    for suf in ['', '_subset']:\n",
    "        train_df = read_file(train_dir / f'{dataset}{suf}_train.csv')\n",
    "        test_df = read_file(test_dir / f'{dataset}_test.csv')\n",
    "        print('train stats:', df_stats(train_df))\n",
    "        print('test stats:', df_stats(test_df))\n",
    "        print('Diversity:', diversity(train_df))\n",
    "        print('Coverage:', coverage(train_df, test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
